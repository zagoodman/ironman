{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Athlete-data\" data-toc-modified-id=\"Athlete-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Athlete data</a></span></li><li><span><a href=\"#Wetsuit-data\" data-toc-modified-id=\"Wetsuit-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Wetsuit data</a></span></li><li><span><a href=\"#Export-html\" data-toc-modified-id=\"Export-html-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Export html</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code scrapes Ironman finisher data at the athlete level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athlete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable names (in order as appear on webpage)\n",
    "\n",
    "dfcolumns = ['athletename', 'athleteid', 'athleteurl', 'bib_n', 'gender', 'division', \\\n",
    "             'divurl', 'konadiv', 'seriesid', 'seriesurl', 'raceid', \\\n",
    "             'raceurl', 'racename', 'raceyear', 'racedate', 'racetype', \\\n",
    "             'konayear', 'konaslots', 'swimtime', 'swimrank', 'swimrankdiv', \\\n",
    "             'swimrankgender', 'biketime', 'bikerank', 'bikerankdiv', \\\n",
    "             'bikerankgender', 'runtime', 'runrank', 'runrankdiv', \\\n",
    "             'runrankgender', 'overalltime', 'overallrank', 'overallrankdiv', \\\n",
    "             'overallrankgender', 'konarankdiv', 'trans1time', 'trans1rank', \\\n",
    "             'trans1rankdiv', 'trans1rankgender', 'trans2time', 'trans2rank', \\\n",
    "             'trans2rankdiv', 'trans2rankgender', 'finish', 'autokonaqual']\n",
    "\n",
    "\n",
    "# define scraping function\n",
    "\n",
    "def get_ath_df(userid):\n",
    "    \"\"\"Returns athlete-race level dataframe\n",
    "    \"\"\"    \n",
    "    print(\"Aquiring data for user: %d       \" % userid, end='\\r')\n",
    "    \n",
    "    # get web response\n",
    "    urlbase = \"https://www.coachcox.co.uk/imstats/athlete/\"\n",
    "    url = urlbase + str(userid)\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "        \n",
    "    # translate to BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # find athlete data\n",
    "    s = soup.find(\"script\", text=re.compile(r\"var imathleteresultstable\"))\n",
    "    try:\n",
    "        s = s.contents[0]\n",
    "    except:\n",
    "        # no data found, return null\n",
    "        return None\n",
    "    \n",
    "    # within the JS script, find the array with relevant data\n",
    "    start = 'var imathleteresultsdata = '\n",
    "    end = ';'\n",
    "    datastring = s[s.find(start)+len(start):s.find(end)]\n",
    "    \n",
    "    # get athlete name\n",
    "    athletename = soup.find(\"title\").contents[0]\n",
    "    athletename = athletename[13:athletename.find(\" Race Results\")]\n",
    "\n",
    "    # turn data into df and clean/label\n",
    "    data = json.loads(datastring)\n",
    "    dfath = pd.DataFrame(data)\n",
    "    dfath.insert(loc=0, column='athletename', value=athletename)\n",
    "    dfath.columns = dfcolumns\n",
    "    dfath.loc[:,'racedate'] = dfath.racedate.apply(lambda x: x['d'])\n",
    "    #dfath.loc[:,'racedate'] = dfath.racedate.apply(lambda x: dt.strptime(x['d'], \"%d %b %Y\"))\n",
    "\n",
    "    return dfath \n",
    "\n",
    "\n",
    "# demonstrate use for one athlete\n",
    "\n",
    "df = get_ath_df(184495)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for a few users and append to one df\n",
    "\n",
    "df_array = []\n",
    "nodata = []\n",
    "\n",
    "for i in range(23786, 23786 + 10):\n",
    "    dfath = get_ath_df(i)\n",
    "    if isinstance(dfath, pd.DataFrame):\n",
    "        df_array.append(dfath)\n",
    "    else:\n",
    "        nodata.append(i)\n",
    "        \n",
    "df_all = pd.concat(df_array, 0)\n",
    "        \n",
    "print(\"\\ndone\\n\")\n",
    "print(\"Total athletes captured: {}\".format(len(df_all.athleteid.unique())))\n",
    "print(\"Total athletes failed: {}\".format(len(nodata)))\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wetsuit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New table with location and date\n",
    "url = \"https://www.roka.com/pages/average-ironman-water-temperatures\"\n",
    "response = requests.get(url)\n",
    "\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "# find wetsuit data\n",
    "wetsuit_data = soup.find_all(\"div\", {'class': 'row_190425_water_temp_chart'})\n",
    "\n",
    "\n",
    "\n",
    "race_data_lst = []\n",
    "for idx, w in enumerate(wetsuit_data):\n",
    "    race_data = w.find_all(\"div\", {'class': 'line_190425_water_temp_chart'})\n",
    "    weather_data = w.find_all(\"div\", {'class': 'small_stratum_bottom_190425_water_temp_chart'})\n",
    "    location_data = w.find_all(\"div\", {'class': 'small_stratum_top_190425_water_temp_chart'})\n",
    "    \n",
    "    \n",
    "    if race_data[3].text.strip() == '|':\n",
    "        suit2 = '/' + race_data[4].text\n",
    "    else:\n",
    "        suit2 = ''\n",
    "    race_data_lst.append(pd.DataFrame(\n",
    "        {'venuename': race_data[0].text,\n",
    "        'watertemp': race_data[1].text,\n",
    "        'suittype': (race_data[2].text + suit2),\n",
    "        'date': weather_data[0].text,\n",
    "        'location':location_data[0].text}, index=[idx]))\n",
    "\n",
    "dfwater = pd.concat(race_data_lst, 0)\n",
    "display(dfwater.head())\n",
    "\n",
    "# lowercase the venue names and suit type names \n",
    "for v in ['venuename', 'suittype']:\n",
    "    dfwater[v] = dfwater[v].apply(lambda x: x.lower())\n",
    "\n",
    "    \n",
    "# convert celsius to farenheight\n",
    "def temp_to_f(x):\n",
    "    '''\n",
    "    Takes a temperature str \"x\" returns the numeric temp in F\n",
    "    '''\n",
    "    if x[-1] == \"C\":\n",
    "        return round(int(x[:2]) * 9/5 + 32, ndigits=1)\n",
    "    return int(x[:2])\n",
    "print(temp_to_f('100°C'))\n",
    "\n",
    "# print cleaned data sample\n",
    "display(dfwater.head())\n",
    "\n",
    "dfwater['watertempavg'] = dfwater.watertemp.apply(lambda x: temp_to_f(x))\n",
    "dfwater.drop('watertemp', 1, inplace=True)\n",
    "\n",
    "# display the top 15 venues that have the closest watertemp to the pro cutoff temp\n",
    "dfwater['pro_diff'] = abs(dfwater.watertempavg - 71.42)\n",
    "dfwater = dfwater.sort_values(by='pro_diff', ascending = True)\n",
    "dfwater['tempC'] = (dfwater.watertempavg - 32) * 5/9 \n",
    "display(dfwater.head(60))\n",
    "\n",
    "# save as csv\n",
    "#dfwater.to_csv('~/Documents/dfwater(2).csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize first element of each letter in the venuename and location\n",
    "new_venuename = []\n",
    "new_location = []\n",
    "for index, row in dfwater.iterrows():\n",
    "    venuename = row[\"venuename\"]\n",
    "    new_venuename.append(venuename.title())\n",
    "    location = row[\"location\"]\n",
    "    new_location.append(location.split(\",\")[0].title())\n",
    "\n",
    "dfwater[\"venuename\"] = new_venuename\n",
    "dfwater[\"location\"] = new_location\n",
    "\n",
    "# save as csv\n",
    "dfwater.to_csv('~/Documents/dfwater(3).csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.roka.com/pages/average-ironman-water-temperatures\"\n",
    "response = requests.get(url)\n",
    "\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "# find wetsuit data\n",
    "wetsuit_data = soup.find_all(\"div\", {'class': 'row_190425_water_temp_chart'})\n",
    "\n",
    "race_data_lst = []\n",
    "for idx, w in enumerate(wetsuit_data):\n",
    "    race_data = w.find_all(\"div\", {'class': 'line_190425_water_temp_chart'})\n",
    "    \n",
    "    if race_data[3].text.strip() == '|':\n",
    "        suit2 = '/' + race_data[4].text\n",
    "    else:\n",
    "        suit2 = ''\n",
    "    race_data_lst.append(pd.DataFrame(\n",
    "        {'venuename': race_data[0].text,\n",
    "        'watertemp': race_data[1].text,\n",
    "        'suittype': (race_data[2].text + suit2)}, index=[idx]))\n",
    "\n",
    "    \n",
    "dfwater = pd.concat(race_data_lst, 0)\n",
    "display(dfwater.head())\n",
    "\n",
    "\n",
    "# lowercase the venue names and suit type names \n",
    "for v in ['venuename', 'suittype']:\n",
    "    dfwater[v] = dfwater[v].apply(lambda x: x.lower())\n",
    "\n",
    "    \n",
    "# convert celsius to farenheight\n",
    "def temp_to_f(x):\n",
    "    '''\n",
    "    Takes a temperature str \"x\" returns the numeric temp in F\n",
    "    '''\n",
    "    if x[-1] == \"C\":\n",
    "        return round(int(x[:2]) * 9/5 + 32, ndigits=1)\n",
    "    return int(x[:2])\n",
    "print(temp_to_f('100°C'))\n",
    "\n",
    "# print cleaned data sample\n",
    "display(dfwater.head())\n",
    "\n",
    "dfwater['watertempavg'] = dfwater.watertemp.apply(lambda x: temp_to_f(x))\n",
    "dfwater.drop('watertemp', 1, inplace=True)\n",
    "\n",
    "# display the top 15 venues that have the closest watertemp to the pro cutoff temp\n",
    "dfwater['pro_diff'] = abs(dfwater.watertempavg - 71.42)\n",
    "dfwater = dfwater.sort_values(by='pro_diff', ascending = True)\n",
    "dfwater['tempC'] = (dfwater.watertempavg - 32) * 5/9 \n",
    "display(dfwater.head(60))\n",
    "\n",
    "# display the top 15 venues that have the closest watertemp to the amateur cutoff temp\n",
    "dfwater['amateur_diff'] = abs(dfwater.watertempavg - 76.1)\n",
    "dfwater = dfwater.sort_values(by='amateur_diff', ascending = True)\n",
    "dfwater['tempC'] = (dfwater.watertempavg - 32) * 5/9 \n",
    "display(dfwater.head(15))\n",
    "\n",
    "# check data\n",
    "print(len(dfwater.venuename.unique()))\n",
    "print(dfwater.suittype.value_counts())\n",
    "\n",
    "# plot avg water temp distribution\n",
    "plt.hist(dfwater.watertempavg, bins=20)\n",
    "# optional/illegal cutoff temp (>76.1 wetsuits not allowed)\n",
    "plt.plot([76.1, 76.1], [0, 20])\n",
    "# mandatory/optional cutoff temp (<60.8 wesuits mandatory)\n",
    "plt.plot([60.8, 60.8], [0, 20])\n",
    "\n",
    "\n",
    "# save as csv\n",
    "\n",
    "#dfwater.to_csv('dir/dfwater.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export html\n",
    "\n",
    "Before committing:\n",
    "1. Save nb as html\n",
    "2. Clear nb of output (cell -> All output -> clear)\n",
    "3. Save nb\n",
    "4. Commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook scrape_imdata.ipynb to html\n",
      "[NbConvertApp] Writing 649467 bytes to ../jupyter_html/scrape_imdata.html\n"
     ]
    }
   ],
   "source": [
    "# save html with results\n",
    "\n",
    "!jupyter nbconvert --output-dir='../jupyter_html/' --to html scrape_imdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

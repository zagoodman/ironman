{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Athlete-data\" data-toc-modified-id=\"Athlete-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Athlete data</a></span></li><li><span><a href=\"#Wetsuit-data\" data-toc-modified-id=\"Wetsuit-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Wetsuit data</a></span></li><li><span><a href=\"#Weather-and-water-temp-data\" data-toc-modified-id=\"Weather-and-water-temp-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Weather and water temp data</a></span></li><li><span><a href=\"#Export-html\" data-toc-modified-id=\"Export-html-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Export html</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code scrapes Ironman finisher data at the athlete level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athlete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable names (in order as appear on webpage)\n",
    "\n",
    "dfcolumns = ['athletename', 'athleteid', 'athleteurl', 'bib_n', 'gender', 'division', \\\n",
    "             'divurl', 'konadiv', 'seriesid', 'seriesurl', 'raceid', \\\n",
    "             'raceurl', 'racename', 'raceyear', 'racedate', 'racetype', \\\n",
    "             'konayear', 'konaslots', 'swimtime', 'swimrank', 'swimrankdiv', \\\n",
    "             'swimrankgender', 'biketime', 'bikerank', 'bikerankdiv', \\\n",
    "             'bikerankgender', 'runtime', 'runrank', 'runrankdiv', \\\n",
    "             'runrankgender', 'overalltime', 'overallrank', 'overallrankdiv', \\\n",
    "             'overallrankgender', 'konarankdiv', 'trans1time', 'trans1rank', \\\n",
    "             'trans1rankdiv', 'trans1rankgender', 'trans2time', 'trans2rank', \\\n",
    "             'trans2rankdiv', 'trans2rankgender', 'finish', 'autokonaqual']\n",
    "\n",
    "\n",
    "# define scraping function\n",
    "\n",
    "def get_ath_df(userid):\n",
    "    \"\"\"Returns athlete-race level dataframe\n",
    "    \"\"\"    \n",
    "    print(\"Aquiring data for user: %d       \" % userid, end='\\r')\n",
    "    \n",
    "    # get web response\n",
    "    urlbase = \"https://www.coachcox.co.uk/imstats/athlete/\"\n",
    "    url = urlbase + str(userid)\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "        \n",
    "    # translate to BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # find athlete data\n",
    "    s = soup.find(\"script\", text=re.compile(r\"var imathleteresultstable\"))\n",
    "    try:\n",
    "        s = s.contents[0]\n",
    "    except:\n",
    "        # no data found, return null\n",
    "        return None\n",
    "    \n",
    "    # within the JS script, find the array with relevant data\n",
    "    start = 'var imathleteresultsdata = '\n",
    "    end = ';'\n",
    "    datastring = s[s.find(start)+len(start):s.find(end)]\n",
    "    \n",
    "    # get athlete name\n",
    "    athletename = soup.find(\"title\").contents[0]\n",
    "    athletename = athletename[13:athletename.find(\" Race Results\")]\n",
    "\n",
    "    # turn data into df and clean/label\n",
    "    data = json.loads(datastring)\n",
    "    dfath = pd.DataFrame(data)\n",
    "    dfath.insert(loc=0, column='athletename', value=athletename)\n",
    "    dfath.columns = dfcolumns\n",
    "    dfath.loc[:,'racedate'] = dfath.racedate.apply(lambda x: x['d'])\n",
    "    #dfath.loc[:,'racedate'] = dfath.racedate.apply(lambda x: dt.strptime(x['d'], \"%d %b %Y\"))\n",
    "\n",
    "    return dfath \n",
    "\n",
    "\n",
    "# demonstrate use for one athlete\n",
    "\n",
    "df = get_ath_df(184495)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for a few users and append to one df\n",
    "\n",
    "df_array = []\n",
    "nodata = []\n",
    "\n",
    "for i in range(23786, 23786 + 10):\n",
    "    dfath = get_ath_df(i)\n",
    "    if isinstance(dfath, pd.DataFrame):\n",
    "        df_array.append(dfath)\n",
    "    else:\n",
    "        nodata.append(i)\n",
    "        \n",
    "df_all = pd.concat(df_array, 0)\n",
    "        \n",
    "print(\"\\ndone\\n\")\n",
    "print(\"Total athletes captured: {}\".format(len(df_all.athleteid.unique())))\n",
    "print(\"Total athletes failed: {}\".format(len(nodata)))\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wetsuit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.roka.com/pages/average-ironman-water-temperatures\"\n",
    "response = requests.get(url)\n",
    "\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "# find wetsuit data\n",
    "wetsuit_data = soup.find_all(\"div\", {'class': 'row_190425_water_temp_chart'})\n",
    "\n",
    "race_data_lst = []\n",
    "for idx, w in enumerate(wetsuit_data):\n",
    "    race_data = w.find_all(\"div\", {'class': 'line_190425_water_temp_chart'})\n",
    "    if race_data[3].text.strip() == '|':\n",
    "        suit2 = '/' + race_data[4].text\n",
    "    else:\n",
    "        suit2 = ''\n",
    "    race_data_lst.append(pd.DataFrame(\n",
    "        {'venuename': race_data[0].text,\n",
    "        'watertemp': race_data[1].text,\n",
    "        'suittype': (race_data[2].text + suit2)}, index=[idx]))\n",
    "    \n",
    "dfwater = pd.concat(race_data_lst, 0)\n",
    "display(dfwater.head())\n",
    "\n",
    "\n",
    "# lowercase the venue names and suit type names \n",
    "for v in ['venuename', 'suittype']:\n",
    "    dfwater[v] = dfwater[v].apply(lambda x: x.lower())\n",
    "\n",
    "    \n",
    "# convert celsius to farenheight\n",
    "def temp_to_f(x):\n",
    "    '''\n",
    "    Takes a temperature str \"x\" returns the numeric temp in F\n",
    "    '''\n",
    "    if x[-1] == \"C\":\n",
    "        return round(int(x[:2]) * 9/5 + 32, ndigits=1)\n",
    "    return int(x[:2])\n",
    "print(temp_to_f('100°C'))\n",
    "\n",
    "dfwater['watertempavg'] = dfwater.watertemp.apply(lambda x: temp_to_f(x))\n",
    "dfwater.drop('watertemp', 1, inplace=True)\n",
    "\n",
    "\n",
    "# print cleaned data sample\n",
    "display(dfwater.head())\n",
    "\n",
    "\n",
    "# check data\n",
    "print(len(dfwater.venuename.unique()))\n",
    "print(dfwater.suittype.value_counts())\n",
    "\n",
    "# plot avg water temp distribution\n",
    "plt.hist(dfwater.watertempavg, bins=20)\n",
    "# optional/illegal cutoff temp (>76.1 wetsuits not allowed)\n",
    "plt.plot([76.1, 76.1], [0, 20])\n",
    "# mandatory/optional cutoff temp (<60.8 wesuits mandatory)\n",
    "plt.plot([60.8, 60.8], [0, 20])\n",
    "\n",
    "\n",
    "# save as csv\n",
    "\n",
    "#dfwater.to_csv('dir/dfwater.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather and water temp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Haversine formula function\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    Calculate the surface distance between two points \n",
    "    on the earth from lat/lng pairs\n",
    "    \"\"\"\n",
    "    # convert degrees to radians \n",
    "    lat1, lng1, lat2, lng2 = map(radians, [lat1, lng1, lat2, lng2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlng = lng2 - lng1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlng/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in km\n",
    "    # r = 3956 # Radius of earth in miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import template file\n",
    "\n",
    "df = pd.read_csv('../data/api_temp/race_weather_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOAA NCEI API request\n",
    "# https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation\n",
    "\n",
    "# San Diego lat/lng: 32.7157° N, 117.1611° W\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_temp_data(lat, lng, date):\n",
    "    \"\"\"\n",
    "    Queries the NOAA NCEI API to retrieve surface sea temperature data taken by\n",
    "    a ship or buoy closest to (lat, lng) on date.\n",
    "    \"\"\"\n",
    "\n",
    "    base = 'https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-marine&dataTypes=AIR_TEMP,WIND_SPEED,AMT_PRECIP,SEA_SURF_TEMP&format=json&boundingBox='\n",
    "\n",
    "    # iteratively increase tol until only one station found\n",
    "    tol = 0.5\n",
    "    search_rate = 1.5 # each round update tol to be tol * search_rate\n",
    "    ub = 5 # upper bound - stop searching if this level exceeded\n",
    "    while True:\n",
    "        # print current level for tol\n",
    "        print(tol)\n",
    "\n",
    "        # set bounding box bounds around swim start lat/lng pair\n",
    "        nb = lat + tol\n",
    "        wb = lng - tol # TODO should we multiply by 1/2? I think not\n",
    "        sb = lat - tol\n",
    "        eb = lng + tol\n",
    "\n",
    "        # compose URL\n",
    "        url = base + str(nb) + ',' + str(wb) + ',' + str(sb) + ',' + str(eb) + \\\n",
    "            '&startDate=' + startdate + '&endDate=' + enddate\n",
    "\n",
    "        # request json object, convert to data frame object\n",
    "        resp = requests.get(url=url)\n",
    "        #print(resp.json())\n",
    "        dftmp = pd.DataFrame(resp.json())\n",
    "        dftmp.columns = [x.lower() for x in dftmp.columns]\n",
    "\n",
    "        # keep only obs with temperature observed\n",
    "        if len(dftmp) > 0:\n",
    "            dftmp = dftmp.loc[dftmp.sea_surf_temp.notnull()]\n",
    "\n",
    "        # case with no stations\n",
    "        if len(dftmp) == 0:\n",
    "            tol *= search_rate\n",
    "            if tol > ub:\n",
    "                print(\"Failed to find close enough station.\")\n",
    "                break\n",
    "\n",
    "        # case with at least one observation\n",
    "        else:\n",
    "            # get closest buoy/ship\n",
    "            dftmp['dist'] = dftmp.apply(lambda x: haversine(float(x.latitude), float(x.longitude), lat, lng), 1)\n",
    "            dftmp = dftmp.loc[dftmp.dist == min(dftmp.dist), :]\n",
    "\n",
    "            # in case multiple observations, take temp closest to 5AM (approx when water temp taken for races)\n",
    "            dftmp['timediff'] = abs(pd.to_datetime(dftmp.date).apply(lambda x: x.hour + x.minute/60 - 5))\n",
    "            dftmp = dftmp.loc[dftmp.timediff == min(dftmp.timediff), :]\n",
    "            break\n",
    "\n",
    "    return dftmp\n",
    "\n",
    "# San Diego\n",
    "dftmp = get_temp_data(32.716, -117.1611, '2017-12-20')\n",
    "display(dftmp)\n",
    "\n",
    "# Mar Del Plata\n",
    "dftmp = get_temp_data(-38.005, -57.539, '2017-12-20')\n",
    "display(dftmp)\n",
    "\n",
    "# sea_surf_temp is degrees celsius * 10, I believe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export html\n",
    "\n",
    "Before committing:\n",
    "1. Save nb as html\n",
    "2. Clear nb of output (cell -> All output -> clear)\n",
    "3. Save nb\n",
    "4. Commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html with results\n",
    "\n",
    "!jupyter nbconvert --output-dir='../jupyter_html/' --to html scrape_imdata.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
